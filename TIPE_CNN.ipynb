{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d5af979-4cea-4fb5-9ca7-7eac955b4648",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CheXpert Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4decf8cc-51a2-456c-a447-f3e7ed0dcd35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba8935e5-ddde-4ae9-805a-e90b7910976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#import torch\n",
    "#import torchvision\n",
    "#import torchsummary\n",
    "#from torch.autograd import Variable\n",
    "#import onnx\n",
    "#from onnx_tf.backend import prepare\n",
    "#from pytorch2keras import pytorch_to_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf6c59c-db6b-4b8b-a33a-cf56793272d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62086749-98be-4c7d-82c1-c1ee06085602",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6364b5f2-d7ec-48c7-b049-fed71bb61a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.0001\n",
    "max_data = 50000\n",
    "\n",
    "dataset_dir = \"D:/Arthur/TIPE/data\"\n",
    "img_size = (224, 224, 1)\n",
    "big_batch_size = 320\n",
    "\n",
    "# Classes (only 5 of them)\n",
    "classes = [\"Cardiomegaly\", \n",
    "           \"Edema\",\n",
    "           \"Consolidation\", \n",
    "           \"Atelectasis\", \n",
    "           \"Pleural Effusion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a669eca5-8661-48b2-8fc6-f71c6a8c36dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942f5e22-97da-45ed-b277-2f3bd980d7e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Fetching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dd2118-1d17-4d4d-b3fc-61d70fb0c3e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 'deprecated' - Load the entire dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6badc6f5-ae32-4d22-80e2-094a18b60393",
   "metadata": {
    "tags": []
   },
   "source": [
    "Récupère les images du dataset, les labels stockés dans un .csv, puis met tout en forme.\n",
    "Les images sont standardisées en format et en valeurs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "60eea8ed-92d6-4f19-ae96-b3b5f8e3f40b",
   "metadata": {},
   "source": [
    "max_size = 10000\n",
    "\n",
    "# Récupère les fichiers csv\n",
    "csv_train = pd.read_csv(\"{}/train.csv\".format(dataset_dir), index_col=0)\n",
    "csv_test = pd.read_csv(\"{}/valid.csv\".format(dataset_dir), index_col=0)\n",
    "\n",
    "size_train, size_valid = len(csv_train), len(csv_test)\n",
    "\n",
    "images_train = []\n",
    "targets_train = []\n",
    "images_test = []\n",
    "targets_test = []\n",
    "    \n",
    "### Créer les arrays d'images et de targets associées ###\n",
    "to_do = max_size + size_valid - 1\n",
    "i = 0\n",
    "\n",
    "# Train\n",
    "for path in csv_train.index:\n",
    "    \n",
    "    # On ne traite que les images frontales\n",
    "    if csv_train.loc[path][\"Frontal/Lateral\"] == \"Frontal\":\n",
    "    \n",
    "        img_path = \"{}/{}\".format(dataset_dir.split(\"CheXpert-v1.0-small\")[0], path).replace(\"/\", \"\\\\\")\n",
    "\n",
    "        # Redimensionne les images\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((img_size[0], img_size[1]), Image.ANTIALIAS)\n",
    "        img = np.asarray(img, dtype=\"float32\") # float32 pour conv\n",
    "        \n",
    "        images_train.append(img)\n",
    "\n",
    "        target = np.array([1 if csv_train.loc[path][c] == 1.0 else 0 for c in classes])\n",
    "        targets_train.append(target)\n",
    "                \n",
    "        \n",
    "    # Barre de chargement\n",
    "    done = i/to_do\n",
    "    print(\"Fetching : {}% |{}| {}/{}\".format(int(done*1000)/10, \"#\"*int(done*20) + \"-\"*(20-int(done*20)), i, to_do), end=\"\\r\")\n",
    "    i+=1\n",
    "    \n",
    "    if i >=max_size:\n",
    "        break\n",
    "\n",
    "# Test\n",
    "for path in csv_test.index:\n",
    "    \n",
    "    # On ne traite que les images frontales\n",
    "    if csv_test.loc[path][\"Frontal/Lateral\"] == \"Frontal\":\n",
    "    \n",
    "        img_path = \"{}/{}\".format(dataset_dir.split(\"CheXpert-v1.0-small\")[0], path).replace(\"/\", \"\\\\\")\n",
    "\n",
    "        # Redimensionne les images\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((img_size[0], img_size[1]), Image.ANTIALIAS)\n",
    "        img = np.asarray(img, dtype=\"float32\") # float32 pour conv\n",
    "        \n",
    "        images_test.append(img)\n",
    "\n",
    "        target = np.array([1 if csv_test.loc[path][c] == 1.0 else 0 for c in classes])\n",
    "        targets_test.append(target)\n",
    "                \n",
    "\n",
    "    # Barre de chargement\n",
    "    done = i/to_do\n",
    "    print(\"Fetching : {}% |{}| {}/{}\".format(int(done*1000)/10, \"#\"*int(done*20) + \"-\"*(20-int(done*20)), i, to_do), end=\"\\r\")\n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "\n",
    "### Pré-processing des données ###\n",
    "print(\"Preprocessing...\", end=\"\\r\")\n",
    "\n",
    "images_train = np.array(images_train)\n",
    "targets_train = np.array(targets_train)\n",
    "images_test = np.array(images_test)\n",
    "targets_test = np.array(targets_test)\n",
    "\n",
    "\n",
    "# Normalisation des données\n",
    "scaler = StandardScaler()\n",
    "scaled_images_train = scaler.fit_transform(images_train.reshape(-1, img_size[0]*img_size[1]*img_size[2]))\n",
    "scaled_images_test = scaler.transform(images_test.reshape(-1, img_size[0]*img_size[1]*img_size[2]))\n",
    "\n",
    "del images_train\n",
    "del images_test\n",
    "\n",
    "scaled_images_train = scaled_images_train.reshape(-1, img_size[0], img_size[1], img_size[2])\n",
    "scaled_images_test = scaled_images_test.reshape(-1, img_size[0], img_size[1], img_size[2])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_train, targets_train))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((scaled_images_test, targets_test))\n",
    "\n",
    "print(\"# Data Ready ! #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8697fc3-90e5-4e99-9570-390e2572f152",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 'done' - Preprocessing data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14a67d19-59b0-4729-8aac-2062c6284e07",
   "metadata": {},
   "source": [
    "### Loading data ###\n",
    "\n",
    "csv_train = pd.read_csv(\"{}/CheXpert-v1.0-small/train.csv\".format(dataset_dir), index_col=0)\n",
    "csv_train = csv_train.loc[csv_train[\"Frontal/Lateral\"] == \"Frontal\"]\n",
    "\n",
    "csv_valid = pd.read_csv(\"{}/CheXpert-v1.0-small/valid.csv\".format(dataset_dir), index_col=0)\n",
    "csv_valid = csv_valid.loc[csv_valid[\"Frontal/Lateral\"] == \"Frontal\"]\n",
    "\n",
    "# We discard data with an uncertainty label\n",
    "for c in classes:\n",
    "    csv_train.drop(csv_train[csv_train[c] == -1.0].index, inplace=True)\n",
    "    csv_valid.drop(csv_valid[csv_valid[c] == -1.0].index, inplace=True)\n",
    "\n",
    "size_train, size_valid = len(csv_train), len(csv_valid)\n",
    "\n",
    "images_train = np.zeros((size_train, img_size[0], img_size[1], img_size[2]), dtype=\"float32\")\n",
    "targets_train = np.zeros((size_train, len(classes)), dtype=\"float32\")\n",
    "images_valid = np.zeros((size_valid, img_size[0], img_size[1], img_size[2]), dtype=\"float32\")\n",
    "targets_valid = np.zeros((size_valid, len(classes)), dtype=\"float32\")\n",
    "    \n",
    "### Loads the dataset as arrays ###\n",
    "to_do = size_train + size_valid - 1\n",
    "i = 0\n",
    "\n",
    "# Train\n",
    "for path in csv_train.index:\n",
    "    \n",
    "    img_path = \"{}/{}\".format(dataset_dir.split(\"CheXpert-v1.0-small\")[0], path).replace(\"/\", \"\\\\\")\n",
    "\n",
    "    # Resizing\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((img_size[0], img_size[1]), Image.ANTIALIAS)\n",
    "    img = np.asarray(img, dtype=\"float32\") # float32 for conv\n",
    "    img = img.reshape(img_size[0], img_size[1], img_size[2])\n",
    "\n",
    "    target = np.array([1 if csv_train.loc[path][c] == 1.0 else 0 for c in classes])\n",
    "\n",
    "    targets_train[i] = target\n",
    "    images_train[i] = img\n",
    "        \n",
    "    # Loading bar\n",
    "    done = i/to_do\n",
    "    print(\"Fetching : {}% |{}| {}/{}\".format(int(done*1000)/10, \"#\"*int(done*20) + \"-\"*(20-int(done*20)), i, to_do), end=\"\\r\")\n",
    "    i+=1\n",
    "    \n",
    "# Valid\n",
    "for path in csv_valid.index:\n",
    "    \n",
    "    img_path = \"{}/{}\".format(dataset_dir.split(\"CheXpert-v1.0-small\")[0], path).replace(\"/\", \"\\\\\")\n",
    "\n",
    "    # Resizing\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((img_size[0], img_size[1]), Image.ANTIALIAS)\n",
    "    img = np.asarray(img, dtype=\"float32\") # float32 pour conv\n",
    "    img = img.reshape(img_size[0], img_size[1], img_size[2])\n",
    "    \n",
    "    target = np.array([1 if csv_valid.loc[path][c] == 1.0 else 0 for c in classes])\n",
    "    \n",
    "    targets_valid[i-size_train] = target\n",
    "    images_valid[i-size_train] = img\n",
    "\n",
    "    # Loading bar\n",
    "    done = i/to_do\n",
    "    print(\"Fetching : {}% |{}| {}/{}\".format(int(done*1000)/10, \"#\"*int(done*20) + \"-\"*(20-int(done*20)), i, to_do), end=\"\\r\")\n",
    "    i+=1\n",
    "\n",
    "print()\n",
    "print(\"# Data Loaded #\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09dc9823-bbc4-4957-9dd3-0bae3bc9556a",
   "metadata": {},
   "source": [
    "### Processing data ###\n",
    "\n",
    "# Reshaping 1 (inplace)\n",
    "print(\"Processing : First Reshaping...\")\n",
    "images_train.shape = (-1, img_size[0]*img_size[1]*img_size[2])\n",
    "images_valid.shape = (-1, img_size[0]*img_size[1]*img_size[2])\n",
    "\n",
    "# Normalisation des données (inplace)\n",
    "print(\"Processing : Normalizing...\")\n",
    "m = images_train.mean()\n",
    "std = images_train.std()\n",
    "\n",
    "images_train-=m\n",
    "images_train/=std\n",
    "\n",
    "images_valid-=m\n",
    "images_valid/=std\n",
    "\n",
    "# Reshaping 2 (inplace)\n",
    "print(\"Processing : Second Reshaping...\")\n",
    "images_train.shape = (-1, img_size[0], img_size[1], img_size[2])\n",
    "images_valid.shape = (-1, img_size[0], img_size[1], img_size[2])\n",
    "\n",
    "print(\"# Data Processed #\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6905c735-e658-444d-ba23-f42b0db97a61",
   "metadata": {},
   "source": [
    "### Saving the data on files ###\n",
    "\n",
    "to_do = size_train // big_batch_size\n",
    "\n",
    "# Training data\n",
    "os.chdir(dataset_dir+\"/NPZ/train\")\n",
    "\n",
    "for i in range(to_do):\n",
    "    \n",
    "    big_batch = images_train[i*big_batch_size : (i+1)*big_batch_size]\n",
    "    targets = targets_train[i*big_batch_size : (i+1)*big_batch_size]\n",
    "    \n",
    "    np.savez(\"batch_{}.npz\".format(i), images=big_batch, targets=targets)\n",
    "    \n",
    "    # Loading bar\n",
    "    done = i/to_do\n",
    "    print(\"Saving : {}% |{}| {}/{}\".format(int(done*1000)/10, \"#\"*int(done*20) + \"-\"*(20-int(done*20)), i, to_do), end=\"\\r\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Validation data\n",
    "os.chdir(\"../valid\")\n",
    "\n",
    "np.savez(\"batch_0.npz\", images=images_valid, targets=targets_valid)\n",
    "\n",
    "print(\"### Data Saved ! ###\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ddb925-e1d5-4afa-8b30-5d9e7ce919d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create DataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680d2137-87c0-43fb-a44e-171dfbe19d79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 'deprecated' - Directly loading images"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cb1d6492-10e5-4b77-99be-05aa7742fa58",
   "metadata": {},
   "source": [
    "Le générateur récupère les données par batch lorsqu'il est appelé par model.fit."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4608b4a5-9bc5-4284-a903-a1889ec05b0c",
   "metadata": {},
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Load data directly from the dataset (images).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size, dataset_directory):\n",
    "        self.batch_size = batch_size\n",
    "        self.directory = dataset_directory\n",
    "        \n",
    "        self.csv = pd.read_csv(self.directory, index_col=0)\n",
    "        self.csv = self.csv.loc[self.csv[\"Frontal/Lateral\"] == \"Frontal\"][:max_data]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.csv) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        batch_csv = self.csv[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        images = []\n",
    "        targets = []\n",
    "        \n",
    "        for path in batch_csv.index:\n",
    "            img_path = \"{}/{}\".format(self.directory.split(\"CheXpert-v1.0-small\")[0], path).replace(\"/\", \"\\\\\")\n",
    "\n",
    "            # Images\n",
    "            img = Image.open(img_path)\n",
    "            img = img.resize((img_size[0], img_size[1]), Image.ANTIALIAS)\n",
    "            img = np.asarray(img, dtype=\"float32\") # float32 pour conv\n",
    "            img = img.reshape(img_size[0], img_size[1], img_size[2])\n",
    "            images.append(img)\n",
    "            \n",
    "            # Targets\n",
    "            target = np.array([1 if batch_csv.loc[path][c] == 1.0 else 0 for c in classes])\n",
    "            targets.append(target)\n",
    "\n",
    "        return np.array(images), np.array(targets)\n",
    "\n",
    "TrainingGenerator = DataGenerator(batch_size=batch_size, dataset_directory=dataset_dir+\"/CheXpert-v1.0-small/train.csv\")\n",
    "ValidationGenerator = DataGenerator(batch_size=batch_size, dataset_directory=dataset_dir+\"/CheXpert-v1.0-small/valid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269a758e-bd4b-47fe-85e9-285e3db38c9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Loading *.npz files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8555294-9f83-4632-be1c-dafe02c1af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variante pour les fichiers npz\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Load data from pre-processed *.npz files (normalized, etc.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, batch_size, dataset_directory):\n",
    "        self.batch_size = batch_size\n",
    "        self.directory = dataset_directory\n",
    "        self.list_IDs = os.listdir(self.directory)\n",
    "   \n",
    "    def __len__(self):\n",
    "        return min(len(self.list_IDs) * big_batch_size, max_data) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        big_batch = (index * self.batch_size) // big_batch_size\n",
    "        \n",
    "        np_file = np.load(self.directory+\"/batch_{}.npz\".format(big_batch))\n",
    "        \n",
    "        return np_file[\"images\"][(index*self.batch_size)%big_batch_size : ((index*self.batch_size)%big_batch_size)+self.batch_size], \\\n",
    "               np_file[\"targets\"][(index*self.batch_size)%big_batch_size : ((index*self.batch_size)%big_batch_size)+self.batch_size]\n",
    "        \n",
    "TrainingGenerator = DataGenerator(batch_size=batch_size, dataset_directory=dataset_dir+\"/NPZ/train\")\n",
    "ValidationGenerator = DataGenerator(batch_size=batch_size, dataset_directory=dataset_dir+\"/NPZ/valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef1847-b224-40a3-a53e-178af992a3d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1852259-cb5f-4c9b-9199-856ad8ed3c09",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8439472b-4c17-48f3-b21b-9964c216efff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification_Head(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Classification_Head, self).__init__()\n",
    "        \n",
    "        self.list_layers = [\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(4096, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(4096, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(len(classes), activation='sigmoid')\n",
    "        ]\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        for layer in self.list_layers :\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3d838-942c-454c-9bbe-30669f9ee14f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 'done' - Pretrained Alexnet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ad2e4ac-1f75-4df2-93d7-f68ea9c681b0",
   "metadata": {},
   "source": [
    "# DEPRECATED\n",
    "torch_pretrained = torchvision.models.alexnet()\n",
    "torch_pretrained.load_state_dict(torch.load(\"alexnet.pth\"))\n",
    "#torchsummary.summary(torch_pretrained, (3,224,224))\n",
    "\n",
    "dummy_input = Variable(torch.randn(1, 3, 224, 224))\n",
    "torch.onnx.export(torch_pretrained, dummy_input, \"alexnet_pretrained.onnx\")\n",
    "\n",
    "onnx_pretrained = onnx.load(\"alexnet_pretrained.onnx\")\n",
    "onnx_pretrained = prepare(onnx_pretrained)\n",
    "\n",
    "onnx_pretrained.export_graph('alexnet')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b03ab7f5-eae5-47c3-8dda-8305da1b71ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "torch_pretrained = torchvision.models.alexnet()\n",
    "torch_pretrained.load_state_dict(torch.load(\"pretrained_model/alexnet.pth\"))\n",
    "\n",
    "dummy_input = Variable(torch.randn(1, 3, 224, 224))\n",
    "keras_pretrained = pytorch_to_keras(torch_pretrained, dummy_input, [(3,224,224)], verbose=False, name_policy=\"short\")\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "for layer in keras_pretrained.layers[:-5]: # go through until last layer\n",
    "    model.add(layer)\n",
    "\n",
    "model.summary()\n",
    "model.save(\"pretrained_model/alexnet_pretrained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3218a82e-945a-4406-8186-dccea7188de5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### AlexNet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "326096bd-ef69-473c-82da-1c0846af1817",
   "metadata": {},
   "source": [
    "class AlexNet_CONV(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AlexNet_CONV, self).__init__()\n",
    "        \n",
    "        self.list_layers = [\n",
    "            tf.keras.layers.Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(224, 224, 1)),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    \n",
    "            tf.keras.layers.Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2)),\n",
    "    \n",
    "            tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.MaxPool2D(pool_size=(3,3), strides=(2,2))\n",
    "        ]\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        for layer in self.list_layers :\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8994b9-565c-4200-9e3d-d39810c99b9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### VGG16"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0ec17f2-971a-4e2a-8cdf-eb88e32f7826",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\", input_shape=(224, 224, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
    "    \n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(4096, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(len(classes), activation='sigmoid')\n",
    "], name=\"VGG16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec6c47a-0fb8-4f45-80dd-d339f5ae7dc4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Compile functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "698731b8-2479-4ea4-8a59-e92ab1bf290d",
   "metadata": {
    "tags": []
   },
   "source": [
    "On utilise BinaryCrossentropy car la prédiction finale n'est pas réduite à une seule classe mais bien à plusieurs.\n",
    "\n",
    "L -> nombre de classes\n",
    "p = [p1, ..., pL] -> prediction\n",
    "t = [t1, ..., tL] -> target\n",
    "\n",
    "Loss = -1/L * Somme{ ti*log(pi) + (1-ti)*log(1-pi) }\n",
    "\n",
    "Si ti = 1 -> seul le terme ti*log(pi) compte, on veut maximiser (il faut pi = 1)\n",
    "Si ti = 0 -> seul le terme (1-ti)*log(1-pi) compte, on veut maximiser (il faut 1-pi = 1)\n",
    "\n",
    "On divise par L parce que, à l'inverse de CategoricalCrossentropy, il n'y a pas 1 valeur qui compte dans la somme mais bien toutes.\n",
    "\n",
    "Le -1 permet de diminuer le loss et non de l'augmenter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98895983-0872-4950-a3c8-77f176a1b06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(0.01, decay_steps=1000, decay_rate=0.96, staircase=True)\n",
    "def compiler(model, lr) :\n",
    "    \n",
    "    global loss_object\n",
    "    loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    global optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # For graph mode\n",
    "    global train_loss\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    global valid_loss\n",
    "    valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "    global train_accuracy\n",
    "    train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "    global valid_accuracy\n",
    "    valid_accuracy = tf.keras.metrics.BinaryAccuracy(name='valid_accuracy')\n",
    "\n",
    "    model.compile(\n",
    "            loss=loss_object,\n",
    "            optimizer=optimizer,\n",
    "            metrics=[\"binary_accuracy\"])\n",
    "    \n",
    "    a = model(np.random.random(img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d9a1b-af3e-4143-99c6-b3673eb7529a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fe22a6-32c6-4b1a-8ee5-6739b0923b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644b641d-36a5-4a73-8860-e17c77cdaafb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Step functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ca62e1-b322-4454-bc01-ff386265c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(images, targets):\n",
    "\n",
    "    train_vars = model.trainable_variables\n",
    "    accum_gradient = [tf.zeros_like(var) for var in train_vars]\n",
    "  \n",
    "    for image, target in zip(images, targets) :\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(image)\n",
    "            target.shape = prediction.shape\n",
    "            loss = loss_object(target, prediction)\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(target, prediction) \n",
    "\n",
    "        gradients = tape.gradient(loss, train_vars)\n",
    "        accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "    \n",
    "    accum_gradient = [grad/len(images) for grad in accum_gradient]\n",
    "    \n",
    "    optimizer.apply_gradients(zip(accum_gradient, model.trainable_variables))\n",
    "\n",
    "#@tf.function\n",
    "def valid_step(images, targets):\n",
    "    \n",
    "    predictions = model(images)\n",
    "    loss = loss_object(targets, predictions)\n",
    "    \n",
    "    valid_loss(loss)\n",
    "    valid_accuracy(targets, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cf6c7-9086-4e5b-b157-d003c11c8b69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Boucle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37063635-41d8-45de-98d1-9eb1aec24272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "\n",
    "    b = 0 # Permet de tracker où on en est dans les batchs\n",
    "    history = {'accuracy':[], 'loss':[], 'val_accuracy':[], 'val_loss':[]}\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # TRAINING\n",
    "        for images, targets in TrainingGenerator:\n",
    "\n",
    "            train_step(images, targets)\n",
    "\n",
    "            template = '\\r Batch {}/{}, Loss: {}, Accuracy: {}'\n",
    "            print(template.format(b, len(TrainingGenerator), train_loss.result(), train_accuracy.result()*100), end=\"\")\n",
    "\n",
    "            b += 1\n",
    "\n",
    "        # VALIDATION\n",
    "        for images, targets in ValidationGenerator:\n",
    "            valid_step(images, targets)\n",
    "\n",
    "        template = '\\nEpoch {}, Valid Loss: {}, Valid Accuracy: {}'\n",
    "        print(template.format(\n",
    "            epoch+1,\n",
    "            valid_loss.result(), \n",
    "            valid_accuracy.result()*100)\n",
    "        )\n",
    "        \n",
    "        # On stocke les accumulateurs dans l'history\n",
    "        history['accuracy'].append(train_accuracy.result())\n",
    "        history['loss'].append(train_loss.result())\n",
    "        history['val_accuracy'].append(valid_accuracy.result())\n",
    "        history['val_loss'].append(valid_loss.result())\n",
    "        \n",
    "        # On reset les accumulateurs\n",
    "        valid_loss.reset_states()\n",
    "        valid_accuracy.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        train_loss.reset_states()\n",
    "        b = 0\n",
    "        print(\"\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb559d-bac2-4dba-8071-34ddd03337ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Post-training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101875ed-6957-4e8a-8f3d-4ad18497b1bb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### save_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9eec85d-867e-478d-a9b5-03ad0d881cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training(history) :\n",
    "\n",
    "    os.chdir(dataset_dir+\"/training_sessions\")\n",
    "\n",
    "    # Create a folder for the training session\n",
    "    i=0\n",
    "    error = True\n",
    "    while error:\n",
    "        try:\n",
    "            fd = \"{}_{}k_{}-epochs_{}\".format(model.name, str(max_data)[:-3], str(epochs), str(i))\n",
    "            os.mkdir(\"{}\".format(fd))\n",
    "        except FileExistsError:\n",
    "            i+=1\n",
    "        else:\n",
    "            error=False\n",
    "\n",
    "    os.chdir(dataset_dir+\"/training_sessions/{}\".format(fd))\n",
    "\n",
    "    # Report file\n",
    "    with open(\"training_report.txt\", \"w\") as f:\n",
    "        f.write(\"TRAINING REPORT :\\n\\n\")\n",
    "        f.write(\"epochs = \" + str(epochs))\n",
    "        f.write(\"\\nbatch_size = \" + str(batch_size))\n",
    "        f.write(\"\\nlearning_rate = \" + str(learning_rate))\n",
    "        f.write(\"\\nmax_data = \" + str(max_data))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\nSummary :\\n\")\n",
    "        model.summary(print_fn=lambda x : f.write(\"\\n\" + x))\n",
    "\n",
    "    # Retrieve curves\n",
    "    loss_curve = history[\"loss\"]\n",
    "    acc_curve = history[\"binary_accuracy\"]\n",
    "    loss_val_curve = history[\"val_loss\"]\n",
    "    acc_val_curve = history[\"val_binary_accuracy\"]\n",
    "\n",
    "    # Loss fig\n",
    "    plt.plot(loss_curve, \"r\", label=\"Train\")\n",
    "    plt.plot(loss_val_curve, \"g\", label=\"Validation\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Loss\")\n",
    "    plt.savefig(\"loss.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    # Accuracy fig\n",
    "    plt.plot(acc_curve, \"r\", label=\"Train\")\n",
    "    plt.plot(acc_val_curve, \"g\", label=\"Validation\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.savefig(\"accuracy.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    # Save the model\n",
    "    model.save(\"model\")\n",
    "\n",
    "    print(\"# Model Saved ! #\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649696a-5ec6-4d6b-8c0f-38684abdb827",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### visualize_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abdffdcc-bab2-4960-bfc7-9b51b846f35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_training(history) :\n",
    "    \n",
    "    # Retrieve curves\n",
    "    loss_curve = history[\"loss\"]\n",
    "    acc_curve = history[\"binary_accuracy\"]\n",
    "\n",
    "    loss_val_curve = history[\"val_loss\"]\n",
    "    acc_val_curve = history[\"val_binary_accuracy\"]\n",
    "\n",
    "    ### Plot the fig ###\n",
    "\n",
    "    # Loss\n",
    "    plt.plot(loss_curve, \"r\", label=\"Train\")\n",
    "    plt.plot(loss_val_curve, \"g\", label=\"Validation\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Loss\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.plot(acc_curve, \"r\", label=\"Train\")\n",
    "    plt.plot(acc_val_curve, \"g\", label=\"Validation\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Accuracy\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7db9d9-b0bb-4b72-855d-0cee929a38f5",
   "metadata": {},
   "source": [
    "# LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e71c00-ae29-464b-8be7-b1eb3c35a8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Alexnet\n",
    "pretrained_extractor = tf.keras.models.load_model(\"pretrained_model/alexnet_pretrained.h5\")\n",
    "\n",
    "# VGG16\n",
    "#pretrained_extractor = tf.keras.applications.vgg16.VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "\n",
    "classification_head = Classification_Head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4cdf58-096b-4b06-991f-4ca03afa90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        super(model, self).__init__(name=name)\n",
    "        \n",
    "        self.base = pretrained_extractor\n",
    "        self.base.trainable = False\n",
    "        \n",
    "        self.classification = classification_head\n",
    "        \n",
    "        self.base.trainable = False\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        x = tf.image.grayscale_to_rgb(x)\n",
    "        x = tf.reshape(x, shape=(-1,3,224,224))\n",
    "        x = self.base(x)\n",
    "        x = self.classification(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f09290a-4224-4d79-96d7-50ee232fb3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model(name=\"AlexNet\")\n",
    "compiler(model, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "847044e6-091a-4e1a-bdcc-c1a15ef6e624",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1562/1562 [==============================] - 312s 199ms/step - loss: 0.4605 - binary_accuracy: 0.8001 - val_loss: 0.5445 - val_binary_accuracy: 0.7307\n",
      "Epoch 2/10\n",
      "1562/1562 [==============================] - 294s 188ms/step - loss: 0.4345 - binary_accuracy: 0.8092 - val_loss: 0.5499 - val_binary_accuracy: 0.7277\n",
      "Epoch 3/10\n",
      "1562/1562 [==============================] - 287s 184ms/step - loss: 0.4270 - binary_accuracy: 0.8118 - val_loss: 0.5433 - val_binary_accuracy: 0.7228\n",
      "Epoch 4/10\n",
      "1562/1562 [==============================] - 288s 184ms/step - loss: 0.4225 - binary_accuracy: 0.8143 - val_loss: 0.5448 - val_binary_accuracy: 0.7297\n",
      "Epoch 5/10\n",
      "1562/1562 [==============================] - 306s 196ms/step - loss: 0.4183 - binary_accuracy: 0.8162 - val_loss: 0.5396 - val_binary_accuracy: 0.7297\n",
      "Epoch 6/10\n",
      "1562/1562 [==============================] - 282s 181ms/step - loss: 0.4144 - binary_accuracy: 0.8179 - val_loss: 0.5395 - val_binary_accuracy: 0.7327\n",
      "Epoch 7/10\n",
      "1562/1562 [==============================] - 290s 185ms/step - loss: 0.4108 - binary_accuracy: 0.8192 - val_loss: 0.5461 - val_binary_accuracy: 0.7337\n",
      "Epoch 8/10\n",
      "1562/1562 [==============================] - 298s 191ms/step - loss: 0.4070 - binary_accuracy: 0.8210 - val_loss: 0.5495 - val_binary_accuracy: 0.7297\n",
      "Epoch 9/10\n",
      "1562/1562 [==============================] - 283s 181ms/step - loss: 0.4041 - binary_accuracy: 0.8222 - val_loss: 0.5458 - val_binary_accuracy: 0.7307\n",
      "Epoch 10/10\n",
      "1562/1562 [==============================] - 300s 192ms/step - loss: 0.4005 - binary_accuracy: 0.8240 - val_loss: 0.5308 - val_binary_accuracy: 0.7327\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(TrainingGenerator, batch_size=batch_size, epochs=epochs, validation_data=ValidationGenerator)\n",
    "\n",
    "save_training(history.history)\n",
    "visualize_training(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a902a-77c1-4d76-a995-57ed7b59f7c7",
   "metadata": {},
   "source": [
    "### Loading previsous models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24c67390-6832-4f91-92ed-6e25465315fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loaded_model = tf.keras.models.load_model(\"*.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
